{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "activated-wells",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [Contents](Index.ipynb) | [Join Merge and Concatenate](02-join-merge-and-concatenate.ipynb) >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-portland",
   "metadata": {},
   "source": [
    "# Pandas: Labeled Column-oriented Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-interference",
   "metadata": {},
   "source": [
    "Pandas is a much newer package than NumPy, and is in fact built on top of it.\n",
    "What Pandas provides is a labeled interface to multi-dimensional data, in the form of a DataFrame object that will feel very familiar to users of R and related languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-third",
   "metadata": {},
   "source": [
    "Moreover, Pandas gained a lot of popularity by data science/analysts community in the past years. You can see by the graph below, that comparing only Pandas with other tools for data manipulation, the usage cases keeps increasing in **Stack Overflow**. It specifies that it is a very popular tool for data manipulation with tons of different cases online. One of the benefits of Pandas:\n",
    "- Easy to manipulate data and track the code\n",
    "- Based on Python which is very easy to use and has very powerfull and rich libraries/tools\n",
    "- Well written documentation with tons of support online\n",
    "- Probably most of the cases you will face to handle will Pandas, will be online already with the solution\n",
    "- Support a lot of mathematical operations and differnet data types to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-consequence",
   "metadata": {},
   "source": [
    "<img src=\"../fig/pandas_vs_rest.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-manual",
   "metadata": {},
   "source": [
    "Lets look how Pandas DataFrames in Python looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "finnish-bronze",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  value\n",
       "0     A      1\n",
       "1     B      2\n",
       "2     C      3\n",
       "3     A      4\n",
       "4     B      5\n",
       "5     C      6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'label': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
    "                   'value': [1, 2, 3, 4, 5, 6]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-recording",
   "metadata": {},
   "source": [
    "The Pandas interface allows you to do things like select columns by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-castle",
   "metadata": {},
   "source": [
    "Apply string operations across string entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-royal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['label'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-jacksonville",
   "metadata": {},
   "source": [
    "Apply aggregates across numerical entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-superior",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['value'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-papua",
   "metadata": {},
   "source": [
    "And, perhaps most importantly, do efficient database-style joins and groupings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('label').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-walter",
   "metadata": {},
   "source": [
    "Here in one line we have computed the sum of all objects sharing the same label, something that is much more verbose (and much less efficient) using tools provided in Numpy and core Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-temperature",
   "metadata": {},
   "source": [
    "## Load data from Databases, Excel, SAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-hollow",
   "metadata": {},
   "source": [
    "Here you will learn how to load a data from Databases and excel and have a nice dataframe with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-nigeria",
   "metadata": {},
   "source": [
    "### Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-frame",
   "metadata": {},
   "source": [
    "You can extract data from databasesby writing the same SQL queries, where the pandas backend fetches the data outcome.\n",
    "\n",
    "The are many different ways to coonect to a database from Python such as ``pyodbc``, ``SQLAlchemy`` and ``sqlite3``, but the idea with ``pandas`` is always the same: no matter that backend you use, you first need to import Python SQL backend library of choice, then create a connection to your database and finally provide that connection to one of pandas SQL reading methods.\n",
    "\n",
    "In today's example we are going to use ``sqlite3`` backend and a fake database which resides within our working directory named ``airline.db``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "casual-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "import pandas as pd\n",
    "\n",
    "conn = sql.connect(\"airline.db\") # provide connection to database from the database backed library; in this case it is sqlite3\n",
    "df = pd.read_sql(\"SELECT * FROM airline\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-corporation",
   "metadata": {},
   "source": [
    "You can also use ``pd.read_sql_table()`` method to read the whole table completely, in that case you need to provide table name and database connector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-arthritis",
   "metadata": {},
   "source": [
    "We can use command .head(5) to inspect how the first 5 rows looks like. It will us to get a quick general view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "upset-worship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>SCHED_DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>ARR_TIME</th>\n",
       "      <th>SCHED_ARR_TIME</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>CARRIER</th>\n",
       "      <th>FLIGHT</th>\n",
       "      <th>TAILNUM</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>MINUTE</th>\n",
       "      <th>TIME_HOUR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>517.0</td>\n",
       "      <td>515</td>\n",
       "      <td>2.0</td>\n",
       "      <td>830.0</td>\n",
       "      <td>819</td>\n",
       "      <td>11.0</td>\n",
       "      <td>UA</td>\n",
       "      <td>1545</td>\n",
       "      <td>N14228</td>\n",
       "      <td>EWR</td>\n",
       "      <td>IAH</td>\n",
       "      <td>227.0</td>\n",
       "      <td>1400</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>2013-01-01T10:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>533.0</td>\n",
       "      <td>529</td>\n",
       "      <td>4.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>830</td>\n",
       "      <td>20.0</td>\n",
       "      <td>UA</td>\n",
       "      <td>1714</td>\n",
       "      <td>N24211</td>\n",
       "      <td>LGA</td>\n",
       "      <td>IAH</td>\n",
       "      <td>227.0</td>\n",
       "      <td>1416</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>2013-01-01T10:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>623.0</td>\n",
       "      <td>627</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>932</td>\n",
       "      <td>1.0</td>\n",
       "      <td>UA</td>\n",
       "      <td>496</td>\n",
       "      <td>N459UA</td>\n",
       "      <td>LGA</td>\n",
       "      <td>IAH</td>\n",
       "      <td>229.0</td>\n",
       "      <td>1416</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>2013-01-01T11:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>728.0</td>\n",
       "      <td>732</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1041.0</td>\n",
       "      <td>1038</td>\n",
       "      <td>3.0</td>\n",
       "      <td>UA</td>\n",
       "      <td>473</td>\n",
       "      <td>N488UA</td>\n",
       "      <td>LGA</td>\n",
       "      <td>IAH</td>\n",
       "      <td>238.0</td>\n",
       "      <td>1416</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>2013-01-01T12:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>739.0</td>\n",
       "      <td>739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1104.0</td>\n",
       "      <td>1038</td>\n",
       "      <td>26.0</td>\n",
       "      <td>UA</td>\n",
       "      <td>1479</td>\n",
       "      <td>N37408</td>\n",
       "      <td>EWR</td>\n",
       "      <td>IAH</td>\n",
       "      <td>249.0</td>\n",
       "      <td>1400</td>\n",
       "      <td>7</td>\n",
       "      <td>39</td>\n",
       "      <td>2013-01-01T12:00:00Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  YEAR  MONTH  DAY  DEP_TIME  SCHED_DEP_TIME  DEP_DELAY  ARR_TIME  \\\n",
       "0      0  2013      1    1     517.0             515        2.0     830.0   \n",
       "1      1  2013      1    1     533.0             529        4.0     850.0   \n",
       "2      2  2013      1    1     623.0             627       -4.0     933.0   \n",
       "3      3  2013      1    1     728.0             732       -4.0    1041.0   \n",
       "4      4  2013      1    1     739.0             739        0.0    1104.0   \n",
       "\n",
       "   SCHED_ARR_TIME  ARR_DELAY CARRIER  FLIGHT TAILNUM ORIGIN DEST  AIR_TIME  \\\n",
       "0             819       11.0      UA    1545  N14228    EWR  IAH     227.0   \n",
       "1             830       20.0      UA    1714  N24211    LGA  IAH     227.0   \n",
       "2             932        1.0      UA     496  N459UA    LGA  IAH     229.0   \n",
       "3            1038        3.0      UA     473  N488UA    LGA  IAH     238.0   \n",
       "4            1038       26.0      UA    1479  N37408    EWR  IAH     249.0   \n",
       "\n",
       "   DISTANCE  HOUR  MINUTE             TIME_HOUR  \n",
       "0      1400     5      15  2013-01-01T10:00:00Z  \n",
       "1      1416     5      29  2013-01-01T10:00:00Z  \n",
       "2      1416     6      27  2013-01-01T11:00:00Z  \n",
       "3      1416     7      32  2013-01-01T12:00:00Z  \n",
       "4      1400     7      39  2013-01-01T12:00:00Z  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-happening",
   "metadata": {},
   "source": [
    "You can notice that there is nothing much to do with pandas to convert into easy managable dataframe. This is an advantegous of pandas, to not be misleaded by a code while performing rich operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-american",
   "metadata": {},
   "source": [
    "As well we can inspect datatypes of SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "flush-mercy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index               int64\n",
       "YEAR                int64\n",
       "MONTH               int64\n",
       "DAY                 int64\n",
       "DEP_TIME          float64\n",
       "SCHED_DEP_TIME      int64\n",
       "DEP_DELAY         float64\n",
       "ARR_TIME          float64\n",
       "SCHED_ARR_TIME      int64\n",
       "ARR_DELAY         float64\n",
       "CARRIER            object\n",
       "FLIGHT              int64\n",
       "TAILNUM            object\n",
       "ORIGIN             object\n",
       "DEST               object\n",
       "AIR_TIME          float64\n",
       "DISTANCE            int64\n",
       "HOUR                int64\n",
       "MINUTE              int64\n",
       "TIME_HOUR          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-shepherd",
   "metadata": {},
   "source": [
    "### Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-linux",
   "metadata": {},
   "source": [
    "Here you can extract data from excel file, mostly often from the .csv type of format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv('Data_Extension.csv') \n",
    "df_csv.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-vehicle",
   "metadata": {},
   "source": [
    "The error says that \"No such file or directory: 'Data_Extension.csv\", which clearly we have to double check if our file is in the same directory or there is some typo. In this case we have a typo, let's write again a name of the .csv file which is **D_Data_Extension.csv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv('D_Data_Extension.csv') \n",
    "df_csv.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-nature",
   "metadata": {},
   "source": [
    "The file is correct, but we can see that the data in csv was not separated, as we want to have to separated column seperated by **\";\"**. Let's try to include separate option. Data will be separated by semicoloms as given in function **sep=';'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv('D_Data_Extension.csv', sep=';') \n",
    "df_csv.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-pakistan",
   "metadata": {},
   "source": [
    "As we can see, the separation part was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-anxiety",
   "metadata": {},
   "source": [
    "Clearly the datatypes from csv files are based on object which can be seen as string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-start",
   "metadata": {},
   "source": [
    "### SAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-spoke",
   "metadata": {},
   "source": [
    "Once we know where the SAS dataset is, reading into Python is best (much faster, more flexible and more stable) using pyreadstat NOT pandas. Moreover the outcome of df is based on Pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadstat  # see https://github.com/Roche/pyreadstat for more information\n",
    "\n",
    "df, meta = pyreadstat.read_sas7bdat('test.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-genesis",
   "metadata": {},
   "source": [
    "You can read just the metadata to find what columns are in a file, and then only read the relevant ones.This saves both time and memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, meta = pyreadstat.read_sas7bdat('test.sas7bdat', metadataonly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now read just the relevant columns\n",
    "df, meta = pyreadstat.read_sas7bdat('test.sas7bdat', usecols=['DEP_TIME', 'SCHED_DEP_TIME'])\n",
    "print(f'\\n\\n{df.info(verbose=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-fiber",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [Contents](Index.ipynb) | [Join Merge and Concatenate](02-join-merge-and-concatenate.ipynb) >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
